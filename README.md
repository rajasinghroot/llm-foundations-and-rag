# llm-foundations-and-rag

# ğŸš€ Lightweight Open-Source RAG System

## ğŸ“Œ Overview

This project demonstrates how to build a lightweight, open-source **Retrieval-Augmented Generation (RAG)** system using:

- **FastAPI** â€“ API framework  
- **FAISS** â€“ Vector database for similarity search  
- **SentenceTransformers (all-MiniLM-L6-v2)** â€“ Embedding model  
- **FLAN-T5 Small** â€“ Open-source LLM for answer generation  
- **PDF & DOCX Support** â€“ Document ingestion  

âš¡ **No paid APIs required.**

---

## âœ… Features

The system allows users to:

- ğŸ“„ Upload PDF/DOCX documents  
- ğŸ”„ Convert documents into semantic vector embeddings  
- ğŸ” Perform similarity search using FAISS  
- ğŸ¤– Generate contextual answers using an open-source LLM  

---

## ğŸ§  RAG Pipeline Flow

1. ğŸ“„ Document Upload  
2. ğŸ” Text Extraction (PDF/DOCX)  
3. âœ‚ï¸ Token-Based Chunking (300 tokens, 50 overlap)  
4. ğŸ§  Embedding Generation (MiniLM â€“ 384 dim)  
5. ğŸ—„ï¸ FAISS Vector Storage (IndexFlatL2)  
6. â“ User Query  
7. ğŸ“Œ Query Embedding  
8. ğŸ¯ Top-K Semantic Retrieval  
9. ğŸ¤– LLM Answer Generation (FLAN-T5 Small)  
10. âœ… Final Response  



---

## âš™ï¸ Tech Stack

| Component | Tool Used |
|------------|------------|
| API Framework | FastAPI |
| Vector Database | FAISS (IndexFlatL2) |
| Embeddings | all-MiniLM-L6-v2 (384 dimension) |
| LLM | google/flan-t5-small |
| Tokenization | bert-base-uncased |
| File Parsing | pypdf, python-docx |

---

## ğŸ¤– LLM Configuration

### ğŸ”¹ Model Used

```python
LLM_MODEL_NAME: str = "google/flan-t5-small"

## ğŸ—‚ Project Structure

llm-foundations-and-rag/
â”‚
â”œâ”€â”€ app/
â”‚ â”‚
â”‚ â”œâ”€â”€ api/
â”‚ â”‚ â””â”€â”€ v1/
â”‚ â”‚ â”œâ”€â”€ endpoints/
â”‚ â”‚ â”‚ â”œâ”€â”€ init.py
â”‚ â”‚ â”‚ â”œâ”€â”€ search.py
â”‚ â”‚ â”‚ â””â”€â”€ upload.py
â”‚ â”‚ â”‚
â”‚ â”‚ â”œâ”€â”€ init.py
â”‚ â”‚ â””â”€â”€ api.py
â”‚ â”‚
â”‚ â”œâ”€â”€ core/
â”‚ â”‚ â”œâ”€â”€ init.py
â”‚ â”‚ â”œâ”€â”€ documents_loader.py
â”‚ â”‚ â”œâ”€â”€ embeddings.py
â”‚ â”‚ â”œâ”€â”€ generate_chunks.py
â”‚ â”‚ â”œâ”€â”€ llm_generator.py
â”‚ â”‚ â”œâ”€â”€ save_vector.py
â”‚ â”‚ â””â”€â”€ settings.py
â”‚ â”‚
â”‚ â”œâ”€â”€ services/
â”‚ â”‚ â”œâ”€â”€ init.py
â”‚ â”‚ â”œâ”€â”€ search_services.py
â”‚ â”‚ â””â”€â”€ upload_services.py
â”‚ â”‚
â”‚ â”œâ”€â”€ init.py
â”‚ â””â”€â”€ main.py
â”‚
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md


## ğŸ“¥ API Endpoints

### 1ï¸âƒ£ Upload Document

**POST** `/rag/upload`

Uploads a PDF or DOCX file.

#### âœ… Response

```json
{
  "message": "File processed successfully",
  "total_chunks": 4
}


### 2ï¸âƒ£ Query Document

**POST** `/rag/query`

#### ğŸ”¹ Request

```json
{
  "query": "Explain about hospitals in India"
}


### 3ï¸âƒ£ Clear FAISS Index

**POST** `/clear_faiss`

Clears the vector database and stored chunks.

#### âœ… Response

```json
{
  "message": "FAISS index cleared successfully"
}

